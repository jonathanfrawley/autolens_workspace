{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature: Sensitivity Mapping\n",
        "============================\n",
        "\n",
        "Bayesian model comparison allows us to take a dataset, fit it with multiple models and use the Bayesian evidence to\n",
        "quantify which model objectively gives the best-fit following the principles of Occam's Razor.\n",
        "\n",
        "However, a complex model may not be favoured by model comparison not because it is the 'wrong' model, but simply\n",
        "because the dataset being fitted is not of a sufficient quality for the more complex model to be favoured. Sensitivity\n",
        "mapping allows us to address what quality of data would be needed for the more complex model to be favoured or\n",
        "alternatively for what sets of model parameter values it would be favoured for data of a given quality.\n",
        "\n",
        "In order to do this, sensitivity mapping involves us writing a function that uses the model(s) to simulate a dataset.\n",
        "We then use this function to simulate many datasets, for many different models, and fit each dataset using the same\n",
        "model-fitting procedure we used to perform Bayesian model comparison. This allows us to infer how much of a Bayesian\n",
        "evidence increase we should expect for datasets of varying quality and / or models with different parameters.\n",
        "\n",
        "For strong lensing, this process is crucial for dark matter substructure detection, as discussed in the following paper:\n",
        "\n",
        "https://arxiv.org/abs/0903.4752\n",
        "\n",
        "In substructure detection, we scan a strong lens dark matter subhalos by fitting a lens models which include a subhalo.\n",
        "This tells us whether we successfully did detect a subhalo, but it does not tell us where a subhalo has to be located\n",
        "(in relation to the source light) to be detectable, nor does to what masses of subhalo we could actually have made a\n",
        "detection.\n",
        "\n",
        "To answer these questions, we must perform sensitivity mapping, where we simulate many thousands of strong lens images,\n",
        "each of which include a dark matter subhalo at a given (y,x) coordinate at a given mass. We then fit each dataset twice,\n",
        "once with a lens model which does not include a subhalo and once with a lens model that does. If the Bayesian evidence\n",
        "of the model which includes a subhalo is higher than that which does not, then it means a subhalo was detectable!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the strong lens dataset `mass_sie__source_sersic` `from .fits files, which is the dataset we will\n",
        "use to perform lens modeling.\n",
        "\n",
        "This is the same dataset we fitted in the `autolens/intro/fitting.py` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"mass_sie__source_sersic\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", \"no_lens_light\", dataset_name)\n",
        "\n",
        "imaging = al.Imaging.from_fits(\n",
        "    image_path=path.join(dataset_path, \"image.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model-fit also requires a mask defining the regions of the image we fit the lens model to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular(\n",
        "    shape_native=imaging.shape_native, pixel_scales=imaging.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "imaging_plotter = aplt.ImagingPlotter(\n",
        "    imaging=imaging, visuals_2d=aplt.Visuals2D(mask=mask)\n",
        ")\n",
        "imaging_plotter.subplot_imaging()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we set up a simple lens model which we will use in this example to demonstrate sensitivity mapping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings_masked_imaging = al.SettingsMaskedImaging(grid_class=al.Grid2D, sub_size=2)\n",
        "settings = al.SettingsPhaseImaging(settings_masked_imaging=settings_masked_imaging)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin, we define the `base_model` that we use to perform sensitivity mapping. This is the lens model that is f\n",
        "itted to every simulated strong lens without a subhalo, giving us the Bayesian evidence which we compare to the model \n",
        "which includes one!). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base_model = af.CollectionPriorModel(\n",
        "    lens=al.GalaxyModel(redshift=0.5, mass=al.mp.EllipticalIsothermal),\n",
        "    source=al.GalaxyModel(redshift=1.0, bulge=al.lp.EllipticalSersic)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the `perturbation_model`, which is the model component whose parameters we iterate over to perform \n",
        "sensitivity mapping. In this case, this model is a `SphericalNFWMCRLudlow` model and we will iterate over its\n",
        "`centre` and `mass_at_200`. We set it up as a `GalaxyModel` so it has an associated redshift and can be directly\n",
        "passed to the tracer in the simulate function below.\n",
        "\n",
        "Many instances of the `perturbation_model` are created and used to simulate the many strong lens datasets that we fit. \n",
        "However, it is only included in half of the model-fits; corresponding to the lens models which include a dark matter \n",
        "subhalo and whose Bayesian evidence we compare to the simpler model-fits consisting of just the `base_model` to \n",
        "determine if the subhalo was detectable.\n",
        "\n",
        "By fitting both models to every simulated lens, we therefore infer the Bayesian evidence of every model to every \n",
        "dataset. Sensitivity mapping therefore maps out for what values of `centre` and `mass_at_200` in the dark mattter \n",
        "subhalo the model-fit including a subhalo provide higher values of Bayesian evidence than the simpler model-fit (and\n",
        "therefore when it is detectable!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perturbation_model = al.GalaxyModel(redshift=0.5, mass=al.mp.SphericalNFWMCRLudlow)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sensitivity mapping is typically performed over a large range of parameters. However, to make this demonstration quick\n",
        "and clear we are going to fix the `centre` of the subhalo to a value near the Einstein ring of (1.6, 0.0). We will \n",
        "iterate over just two `mass_at_200` values corresponding to subhalos of mass 1e6 and 1e11, of which only the latter\n",
        "will be shown to be detectable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perturbation_model.mass.centre.centre_0 = 1.6\n",
        "perturbation_model.mass.centre.centre_1 =  0.0\n",
        "perturbation_model.mass.redshift_object = 0.5\n",
        "perturbation_model.mass.redshift_source = 1.0\n",
        "perturbation_model.mass.mass_at_200 = af.LogUniformPrior(lower_limit=1e6, upper_limit=1e11)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are performing sensitivity mapping to determine when a subhalo is detectable. However, every simulated dataset must \n",
        "be simulated with a lens model, called the `simulation_instance`. To get this model, we therefore fit the data before \n",
        "performing sensitivity mapping and set the maximum log likelihood result as the `simulation_instance`.\n",
        "\n",
        "This means that it will be used as the model of the lens galaxy and source in the simulation of every dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_base = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"misc\", dataset_name),\n",
        "    name=\"sensitivity_mapping_base\",\n",
        "    n_live_points=50,\n",
        ")\n",
        "\n",
        "phase = al.PhaseImaging(\n",
        "    search=search_base,\n",
        "    galaxies=base_model,\n",
        "    settings=settings,\n",
        ")\n",
        "\n",
        "result = phase.run(dataset=imaging, mask=mask)\n",
        "\n",
        "simulation_instance = result.instance"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now write the `simulate_function`, which takes the `simulation_instance` of our model (defined above) and uses it to \n",
        "simulate a dataset which is subsequently fitted.\n",
        "\n",
        "Note that when this dataset is simulated, the quantity `instance.perturbation` is used in the `simulate_function`.\n",
        "This is an instance of the `SphericalNFWMCRLudlow`, and it is different every time the `simulate_function` is called\n",
        "based on the value of sensitivity being computed. \n",
        "\n",
        "In this example, this `instance.perturbation` corresponds to two different subhalos with values of `mass_at_200` of \n",
        "1e6 MSun and 1e11 MSun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def simulate_function(instance):\n",
        "\n",
        "    \"\"\"\n",
        "    Set up the `Tracer` which is used to simulate the strong lens imaging, which may include the subhalo in\n",
        "    addition to the lens and source galaxy.\n",
        "    \"\"\"\n",
        "    tracer = al.Tracer.from_galaxies(\n",
        "        galaxies=[\n",
        "            instance.galaxies.lens,\n",
        "            instance.perturbation,\n",
        "            instance.galaxies.source,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    Set up the grid, PSF and simulator settings used to simulate imaging of the strong lens. These should be tuned to\n",
        "    match the S/N and noise properties of the observed data you are performing sensitivity mapping on.\n",
        "    \"\"\"\n",
        "    grid = al.Grid2DIterate.uniform(\n",
        "        shape_native=(100, 100),\n",
        "        pixel_scales=0.2,\n",
        "        fractional_accuracy=0.9999,\n",
        "        sub_steps=[2, 4, 8, 16, 24],\n",
        "    )\n",
        "\n",
        "    psf = al.Kernel2D.from_gaussian(\n",
        "        shape_native=(3, 3), sigma=0.1, pixel_scales=grid.pixel_scales\n",
        "    )\n",
        "\n",
        "    simulator = al.SimulatorImaging(\n",
        "        exposure_time=300.0, psf=psf, background_sky_level=0.1, add_poisson_noise=True\n",
        "    )\n",
        "\n",
        "    imaging = simulator.from_tracer_and_grid(tracer=tracer, grid=grid)\n",
        "\n",
        "    \"\"\"\n",
        "    The data generated by the simulate function is that which is fitted, so we should apply the mask for the analysis \n",
        "    here before we return the simulated data.\n",
        "    \"\"\"\n",
        "    return al.MaskedImaging(imaging=imaging, mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each model-fit performed by sensitivity mapping creates a new instance of an `Analysis` class, which contains the\n",
        "data simulated by the `simulate_function` for that model.\n",
        "\n",
        "This requires us to write a wrapper around the PyAutoLens `Analysis` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from astropy import cosmology as cosmo\n",
        "from autolens.pipeline.phase.imaging import analysis as a\n",
        "\n",
        "class Analysis(a.Analysis):\n",
        "\n",
        "    def __init__(self, masked_imaging):\n",
        "\n",
        "        super().__init__(\n",
        "            masked_imaging=masked_imaging,\n",
        "            settings=settings,\n",
        "            cosmology=cosmo.Planck15,\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We next specify the search used to perform each model fit by the sensitivity mapper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"misc\", dataset_name),\n",
        "    name=\"sensitivity_mapping\",\n",
        "    n_live_points=50,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now combine all of the objects created above and perform sensitivity mapping. The inputs to the `Sensitivity`\n",
        "object below are:\n",
        "\n",
        "- simulation_instance: This is an instance of the model used to simulate every dataset that is fitted. In this example it \n",
        "is a lens model that does not include a subhalo, which was inferred by fitting the dataset we perform sensitivity \n",
        "mapping on.\n",
        "\n",
        "- base_model: This is the lens model that is fitted to every simulated dataset, which does not include a subhalo. In \n",
        "this example is composed of an `EllipticalIsothermal` lens and `EllipticalSersic` source.\n",
        "\n",
        "- perturbation_model: This is the extra model component that alongside the `base_model` is fitted to every simulated \n",
        "dataset. In this example it is a `SphericalNFWMCRLudlow` dark matter subhalo.\n",
        "\n",
        "- simulate_function: This is the function that uses the `simulation_instance` and many instances of the `perturbation_model` \n",
        "to simulate many datasets that are fitted with the `base_model` and `base_model` + `perturbation_model`.\n",
        "\n",
        "- analysis_class: The wrapper `Analysis` class that passes each simulated dataset to the `Analysis` class that fits \n",
        "the data.\n",
        "\n",
        "- step_size: The size of steps over which the parameters in the `perturbation_model` are iterated. In this example, \n",
        "`mass_at_200` has a `LogUniformPrior` with lower limit 1e6 and upper limit 1e11, therefore the `step_size` of 0.5 will\n",
        "simulate and fit just 2 datasets where the intensity is 1e6 and 1e11.\n",
        "\n",
        "- number_of_cores: The number of cores over which the sensitivity mapping is performed, enabling parallel processing\n",
        "if set above 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.non_linear.grid import sensitivity as s\n",
        "\n",
        "sensitivity = s.Sensitivity(\n",
        "    search=search,\n",
        "    simulation_instance=simulation_instance,\n",
        "    #  base_model=result.model,\n",
        "    base_model=base_model,\n",
        "    perturbation_model=perturbation_model,\n",
        "    simulate_function=simulate_function,\n",
        "    analysis_class=Analysis,\n",
        "    step_size=0.5,\n",
        "    number_of_cores=2,\n",
        ")\n",
        "\n",
        "sensitivity_result = sensitivity.run()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should now look at the results of the sensitivity mapping in the folder `output/features/sensitivity_mapping`. \n",
        "\n",
        "You will note the following 4 model-fits have been performed:\n",
        "\n",
        " - The `base_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e6` is included.\n",
        "\n",
        " - The `base_model` + `perturbation_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e6` \n",
        " is included.\n",
        "\n",
        " - The `base_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e11` is included.\n",
        "\n",
        " - The `base_model` + `perturbation_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e11` is \n",
        " included.\n",
        "\n",
        "The fit produces a `sensitivity_result`. \n",
        "\n",
        "We are still developing the `SensitivityResult` class to provide a data structure that better streamlines the analysis\n",
        "of results. If you intend to use sensitivity mapping, the best way to interpret the resutls is currently via\n",
        "**PyAutoFit**'s database and `Aggregator` tools. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# print(sensitivity_result.results[0].result.samples.log_evidence)\n",
        "# print(sensitivity_result.results[1].result.samples.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}